#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

## @section Cluster Paramters
## @param cluster_domain_suffix [t#Cluster Domain Suffix] The doamin suffix of the current k8s cluster
cluster_domain_suffix: cluster.local

## @section Dependency Parameters
## @param openstackDepRelease [t#Openstack-dep Release] The release name of openstack-dep chart
openstackDepRelease: "openstack-dependency"
## @param passwordRelease [t#Password Release] The release name of password chart
passwordRelease: "openstack-password"
## @param keystoneRelease [t#keystone Release] The release name of keystone chart
keystoneRelease: "openstack-keystone"

## @section Image Parameters
## @param imageRegistry [t#Image Registry] The registry address of openstack kolla image
imageRegistry: "registry.aliyuncs.com"
## @param imageNamespace [t#Image Namespace] The registry namespace of openstack kolla image
imageNamespace: "kolla-helm"
## @param openstackTag [t#Openstack version] The openstack version
openstackTag: "yoga"
## @param pullPolicy [t#Pull Policy] The image pull policy
pullPolicy: "IfNotPresent"

## @section Deployment Parameters
## @param replicaCount Number of cinder replicas to deploy (requires ReadWriteMany PVC support)
replicaCount: 1
## @param serviceAccountName ServiceAccount name
serviceAccountName: cinder
## @param enableLivenessProbe [t#Enable Liveness Probe] Whether or not enable liveness probe
enableLivenessProbe: true
## @param enableReadinessProbe [t#Enable Readliness Probe] Whether or not enable readiness probe
enableReadinessProbe: true

## @section Cinder Config parameters
## @param db_database [t#Keystone database] The keystone database name
db_username: cinder
## @param db_username [t#Keystone Database User] The keystone database user name
db_database: cinder
## @param enabled_notification [t#Enable Notification] Whether or not enable notification
enabled_notification: false
lvm:
  ## @param lvm.enabled [t#Enable Lvm] Whether or not enable lvm backend
  enabled: true
  ## @param lvm.vg_name [t#Volume group Name] The volume group name used to as the cinder lvm driver's backend
  vg_name: cinder-volumes
  ## @param lvm.create_loop_device [t#Create Loop Device] Whether or not create a loop device for lvm backend. If this set as false, the lvm volume group must be prepared in advance.
  create_loop_device: true
  ## @param lvm.loop_device_name [t# Loop Device Name] The loop device's name
  loop_device_name: /dev/loop0
  ## @param lvm.loop_device_size [t#Loop Device Size] The loop device's size, unit Mb
  loop_device_size: 2048
  ## @param lvm.loop_device_directory [t#Loop Device Directoyr] The host directory save loop device file
  loop_device_directory: /var/lib/kolla-helm/cinder
  ## @param lvm.volume_type [t#Cinder Volume Type] The cinder volume type name corresponding with lvm backend
  volume_type: lvm1
  ## @param lvm.lvm_target_helper [t#Cinder Lvm Target Helper] Target user-land tool to use
  lvm_target_helper: tgtadm
ceph:
  ## @param ceph.enabled [t#Enable Ceph] Whether or not enable ceph backend
  enabled: true
  ## @param ceph.volume_type [t#Cinder Volume Type] The cinder volume type name corresponding with ceph backend
  volume_type: rbd1
  ## @param ceph.poolName [t#Pool Name] The ceph pool name which used to store the cinder volumes
  poolName: volumes
  ## @param ceph.replicatedSize [t#Pool Replicated Size] For a pool based on raw copies, specify the number of copies. A size of 1 indicates no redundancy.
  replicatedSize: 3
  ## @param ceph.failureDomain [t#Pool Failure Domain] The failure domain will spread the replicas of the data across different failure zones
  failureDomain: host
  ## @param ceph.cephClusterNamespace [t#Rook Ceph Cluster Namespace] The k8s namespace of rook cephcluster
  cephClusterNamespace: rook-ceph
  ## @param ceph.cephClusterName [t#Rook Ceph Cluster Name] The rook cephcluster name
  cephClusterName: rook-ceph
  ## @param ceph.cephClientName [t#Rook Ceph Client Name] The name of rook ceph cephclient
  cephClientName: cinder
  backup:
    ## @param ceph.backup.enabled [t#Enable Cinder Backup] Whether or not enable cinder backup feature
    enabled: true
    ## @param ceph.backup.poolName [t#Backup Pool Name] The name of ceph pool used to store cinder volume backups
    poolName: backups
    ## @param ceph.backup.replicatedSize [t#Pool Replicated Size] For a pool based on raw copies, specify the number of copies. A size of 1 indicates no redundancy.
    replicatedSize: 3
    ## @param ceph.backup.failureDomain [t#Pool Failure Domain] The failure domain will spread the replicas of the data across different failure zones
    failureDomain: host

## @section Ingress Parameters
## @param ingress.enabled [t#Ingress] Whether or not create ingress for keystone service
## @param ingress.ingressClass [t#Ingress Class] Ingress Class Name
## @param ingress.path [t#Path Prefix] Ingress will match the path prefix, and forward the matched request to keystone service
ingress:
  enabled: true
  ingressClass: openstack-nginx
  path: volumev3
